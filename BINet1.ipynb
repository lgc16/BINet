{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 此程序对应BINet文章的第一和第二个实验，可用于求解多边形区域上的Laplace方程"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import scipy.special as scp\r\n",
    "from torch.autograd import Variable as v\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 设置边界条件\r\n",
    "k = 1\r\n",
    "def exact_sol(a,b):\r\n",
    "    return torch.exp(k*a)*torch.sin(k*b) #对应文章例1的算例\r\n",
    "    #return 1-abs(b)+1-abs(a)            #对应文章例2的算例"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 设置多边形区域和边界上的节点和边界条件\r\n",
    "# vertex_num  多边形边数\r\n",
    "# vertex      多边形顶点（按逆时针连接）\r\n",
    "# M           在每条边上取的样本点数\r\n",
    "# normal      样本点处的外法向量\r\n",
    "# h           每条边上两点间的距离\r\n",
    "# sample_num  总样本点数即 每条边上样本点数*多边形顶点数\r\n",
    "# sample_x    样本点坐标\r\n",
    "# sample_u    样本点处的函数值，即边界条件\r\n",
    "\r\n",
    "M = 100\r\n",
    "line = (torch.linspace(0,1-1/M,M)).reshape(-1,1)\r\n",
    "vertex_num = 4\r\n",
    "vertex = torch.Tensor([[-1,-1],[1,-1],[1,1],[-1,1],[-1,-1]])\r\n",
    "\r\n",
    "rot = torch.Tensor([[0,-1],[1,0]])  #顺时针旋转90度\r\n",
    "\r\n",
    "normal = torch.zeros(vertex_num,2)  #各边法向量\r\n",
    "h = torch.zeros(vertex_num,1)     #各边小区间单位长度\r\n",
    "for i in range(vertex_num):\r\n",
    "    normal[i,:] = (vertex[i+1,:]-vertex[i,:])@rot\r\n",
    "    normal[i,:] = normal[i,:]/(normal[i,:].norm())\r\n",
    "    h[i] = (vertex[i+1,:]-vertex[i,:]).norm()/M\r\n",
    "\r\n",
    "sample_num = vertex_num*M\r\n",
    "sample_x = torch.zeros(vertex_num*M,2)\r\n",
    "for i in range(vertex_num):\r\n",
    "    sample_x[i*M:i*M+M,:] = vertex[i,:]+(vertex[i+1,:]-vertex[i,:])*line\r\n",
    "sample_u = torch.zeros(sample_num,1)\r\n",
    "for i in range(sample_num):\r\n",
    "    if (i%M != 0) and (i%M != 1) and (i%M != M-1):\r\n",
    "        sample_u[i] = exact_sol(sample_x[i,0],sample_x[i,1])#d0 + b0*sample_x[i,0]+c0*sample_x[i,1]+a0*sample_x[i,0]*sample_x[i,1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 构造积分矩阵\r\n",
    "# G2     基本解在样本点处的外法向导数\r\n",
    "# A2     各积分点的权重\r\n",
    "\r\n",
    "time11 = time.time()\r\n",
    "G2 = torch.zeros(sample_num,sample_num)\r\n",
    "for i in range(sample_num):\r\n",
    "    for j in range(sample_num):\r\n",
    "        r = sample_x[j,:] - sample_x[i,:]\r\n",
    "        d = r.norm()\r\n",
    "        j0 = int(j/M)\r\n",
    "        j1 = int((j-1)/M)%vertex_num\r\n",
    "        G2[i,j] = -1/(2*np.pi)*(r*normal[j0,:]*h[j0]+r*normal[j1,:]*h[j1]).sum()/(2*d*d)\r\n",
    "        \r\n",
    "\r\n",
    "A2 = torch.zeros(sample_num,sample_num)\r\n",
    "for i in range(sample_num):\r\n",
    "    if (i%M != 0) and (i%M != 1) and (i%M != M-1):\r\n",
    "        sample_u[i] = exact_sol(sample_x[i,0],sample_x[i,1])#d0 + b0*sample_x[i,0]+c0*sample_x[i,1]+a0*sample_x[i,0]*sample_x[i,1]\r\n",
    "        for j in range(sample_num):\r\n",
    "            if i==j:\r\n",
    "                A2[i,j] = 1/2\r\n",
    "            else:\r\n",
    "                A2[i,j] = -G2[i,j]\r\n",
    "time12 = time.time()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 网络结构和损失函数\r\n",
    "\r\n",
    "# ResNet结构\r\n",
    "# m 为神经元个数\r\n",
    "class Net(nn.Module):\r\n",
    "  def __init__(self,m,out):\r\n",
    "    super(Net, self).__init__()\r\n",
    "    self.input = nn.Linear(2,m)\r\n",
    "    self.block1=nn.Sequential(\r\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\r\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\r\n",
    "    )\r\n",
    "    self.block2=nn.Sequential(\r\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\r\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\r\n",
    "    )\r\n",
    "    self.block3=nn.Sequential(\r\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\r\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\r\n",
    "    )\r\n",
    "    self.block4=nn.Sequential(\r\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\r\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\r\n",
    "    )\r\n",
    "    self.block5=nn.Sequential(\r\n",
    "      nn.Linear(m,m),nn.ReLU(),\r\n",
    "      nn.Linear(m,m),nn.ReLU(),\r\n",
    "    )\r\n",
    "    self.block6=nn.Sequential(\r\n",
    "      nn.Linear(m,m),nn.ReLU(),\r\n",
    "      nn.Linear(m,m),nn.ReLU(),\r\n",
    "    )\r\n",
    "    self.out = nn.Linear(m,out)\r\n",
    "  def forward(self, x):\r\n",
    "      x = self.input(x)\r\n",
    "      x = self.block1(x) + x\r\n",
    "      x = self.block2(x) + x\r\n",
    "      x = self.block3(x) + x\r\n",
    "      x = self.block4(x) + x\r\n",
    "      x = self.block5(x) + x\r\n",
    "      x = self.block6(x) + x\r\n",
    "      x = self.out(x)\r\n",
    "      return x\r\n",
    "\r\n",
    "#损失函数，本质即MSE loss\r\n",
    "class Green_loss(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "    def forward(self,u_exact,u_Green):\r\n",
    "        return torch.mean(torch.pow((u_exact-u_Green),2)) \r\n",
    "\r\n",
    "net1 = Net(100,1) \r\n",
    "Green_loss_func = Green_loss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#训练过程\r\n",
    "\r\n",
    "# optimizer   优化器\r\n",
    "# Epoch       总训练次数\r\n",
    "# sample_h    样本点处密度函数值h(x)\r\n",
    "# u0          样本点处数值解\r\n",
    "\r\n",
    "optimizer = torch.optim.Adam(net1.parameters(net1),lr=0.001)\r\n",
    "Epoch = 5000\r\n",
    "loss_all = torch.zeros(int(Epoch/100)+1)\r\n",
    "time0 = time.time()\r\n",
    "for epoch in range(Epoch+1):\r\n",
    "    sample_h = net1(sample_x)#这个方法甚至不需要内部点\r\n",
    "    \r\n",
    "    u0 = -(A2@sample_h)\r\n",
    "\r\n",
    "    loss = Green_loss_func(sample_u,u0)\r\n",
    "\r\n",
    "    optimizer.zero_grad()\r\n",
    "    loss.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch%1000==0:\r\n",
    "        print('loss, epoch, computation time:','%.4f'%loss.detach().numpy(),epoch,'%.4f'%(time.time()-time0))\r\n",
    "        time0 = time.time()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss, epoch, computation time: 4.1300 0 1.4791\n",
      "loss, epoch, computation time: 0.0001 1000 16.9806\n",
      "loss, epoch, computation time: 0.0001 2000 17.5142\n",
      "loss, epoch, computation time: 0.0003 3000 12.0488\n",
      "loss, epoch, computation time: 0.0005 4000 13.5667\n",
      "loss, epoch, computation time: 0.0001 5000 12.7240\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#我们将在区域内部 x_in 坐标的点上测试结果的数值精度\r\n",
    "#测试点坐标保存在 outs.txt 中，也可以随机生成\r\n",
    "#sample = 9801\r\n",
    "#with open('outs.txt','r') as f:\r\n",
    "#    data = f.readlines()\r\n",
    "#u_in = torch.zeros(sample,1)\r\n",
    "#x_in = torch.zeros(sample,2)\r\n",
    "#for i in range(sample):\r\n",
    "#    x_in[i,0] = float((data[i].split())[0])\r\n",
    "#    x_in[i,1] = float((data[i].split())[1])\r\n",
    "#    u_in[i] = exact_sol(x_in[i,0],x_in[i,1])\r\n",
    "\r\n",
    "# 随机生成内部的点测试结果数值精度\r\n",
    "# x_in 内部点坐标\r\n",
    "# u_in 内部点对应的精确解\r\n",
    "sample = 1000\r\n",
    "x_in = torch.rand(sample,2)*2-1\r\n",
    "u_in = exact_sol(x_in[:,0],x_in[:,1]).reshape(-1,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# G2_in 为内部点对应的积分点的权重\r\n",
    "G2_in = torch.zeros(sample,sample_num)\r\n",
    "for i in range(sample):\r\n",
    "    for j in range(sample_num):\r\n",
    "        j0 = int(j/M)\r\n",
    "        j1 = int((j-1)/M)%vertex_num\r\n",
    "        r = sample_x[j,:]-x_in[i,:]\r\n",
    "        d = r.norm()\r\n",
    "        G2_in[i,j] = -1/(2*np.pi)*(r*normal[j0,:]*h[j0]+r*normal[j1,:]*h[j1]).sum()/(2*d*d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 积分\r\n",
    "# u_green 为通过BINet算出来的数值解\r\n",
    "u_green = G2_in@sample_h\r\n",
    "\r\n",
    "# 打印相对L2误差\r\n",
    "print('Relaive L2 error:','%.4f'%((u_green-u_in).norm()/u_in.norm()).detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Relaive L2 error: 0.0983\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "7e807f9ddb0496b7851a27b2566c8810a6974b896f5c34231241c2796bf1297c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}